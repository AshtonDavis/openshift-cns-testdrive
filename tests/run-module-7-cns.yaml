---
- name: Deploy RWO Persistent Storage Application
  hosts: localhost
  tags:
    - rwo_example
    - rwo_deploy
  tasks:
    - include_vars: /opt/lab/environment.yml

    - set_fact:
        cns_namespace: "{{ CNS_NAMESPACE }}"
        cns_storageclass: "{{ CNS_STORAGECLASS }}"
        new_cns_nodes_internal_fqdn:
          - "{{NODE4_INTERNAL_FQDN}}"
          - "{{NODE5_INTERNAL_FQDN}}"
          - "{{NODE6_INTERNAL_FQDN}}"
        heketi_resturl: "http://heketi-storage-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}"
        heketi_admin_pw: "{{ HEKETI_ADMIN_PW }}"
        cns_storageclass2: "{{CNS_STORAGECLASS2}}"
        ocp_routing_suffix: "{{OCP_ROUTING_SUFFIX}}"
        node_brick_device: "{{ NODE_BRICK_DEVICE }}"
        node_brick_device2: "{{ NODE_BRICK_DEVICE2 }}"
        node4_internal_fqdn: " {{ NODE4_INTERNAL_FQDN }}"

    - name: login as fancyuser1
      shell: oc login -u 'fancyuser1' -p 'openshift'

    - name: create new project called 'my-database-app'
      shell: oc new-project my-database-app

    - name: Deploy the rails-postgres-persistent template via new-app
      shell: oc new-app rails-pgsql-persistent -p VOLUME_CAPACITY=5Gi

    - name: wait for postgres app to be ready
      shell: oc get rc -o jsonpath='{$.items[?(@.spec.selector.name=="rails-pgsql-persistent")].status.readyReplicas}'
      register: pgsql_app_rc_check
      until: pgsql_app_rc_check.stdout == "1"
      retries: 60
      delay: 10

- name: Verify RWO Persistent Storage Application
  hosts: localhost
  tags:
    - rwo_example
    - rwo_verify
  tasks:
    - include_vars: /opt/lab/environment.yml

    - set_fact:
        cns_namespace: "{{ CNS_NAMESPACE }}"
        cns_storageclass: "{{ CNS_STORAGECLASS }}"
        new_cns_nodes_internal_fqdn:
          - "{{NODE4_INTERNAL_FQDN}}"
          - "{{NODE5_INTERNAL_FQDN}}"
          - "{{NODE6_INTERNAL_FQDN}}"
        heketi_resturl: "http://heketi-storage-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}"
        heketi_admin_pw: "{{ HEKETI_ADMIN_PW }}"
        cns_storageclass2: "{{CNS_STORAGECLASS2}}"
        ocp_routing_suffix: "{{OCP_ROUTING_SUFFIX}}"
        node_brick_device: "{{ NODE_BRICK_DEVICE }}"
        node_brick_device2: "{{ NODE_BRICK_DEVICE2 }}"
        node4_internal_fqdn: " {{ NODE4_INTERNAL_FQDN }}"

    - name: re-login as system:admin
      shell: oc login -u system:admin

    - name: change to project of fancyuser1
      shell:  oc project my-database-app

    - name: check route
      shell: oc get route/rails-pgsql-persistent

    - name: check pvc called 'postgres' exists
      shell: oc get pvc/postgresql -o json
      register: get_postgresql_pvc
      changed_when: false

    - set_fact:
        postgresql_pvc: "{{ get_postgresql_pvc.stdout|from_json }}"

    - name: ensure pvc is bound
      fail:
        msg: pvc is not bound
      when: postgresql_pvc.status.phase != "Bound"

    - name: ensure pvc is issued against CNS
      assert:
        that:
          - postgresql_pvc.metadata.annotations['volume.beta.kubernetes.io/storage-provisioner'] == 'kubernetes.io/glusterfs'
          - postgresql_pvc.spec.storageClassName == cns_storageclass
        msg: pvc is not served from StorageClass {{ cns_storageclass }}

    - name: check pv is exists
      shell: oc get pv/{{ postgresql_pvc.spec.volumeName }} -o json
      register: get_postgresql_pv
      changed_when: false

    - set_fact:
        postgresql_pv: "{{ get_postgresql_pv.stdout|from_json }}"

    - name: check pv is provided by CNS
      assert:
        that:
          - postgresql_pv.metadata.annotations['pv.kubernetes.io/provisioned-by'] == 'kubernetes.io/glusterfs'
          - postgresql_pv.spec.storageClassName == cns_storageclass
        msg: pv is not provided by {{ cns_storageclass }}

    - name: get volume path of pv for app
      shell: oc get pv $(oc get pvc -n my-database-app postgresql -o jsonpath='{.spec.volumeName}') -o jsonpath='{.spec.glusterfs.path}'
      register: pv_volume_path

    - name: check gluster volume of pv
      shell: oc exec -n {{ cns_namespace }} $(oc get pods -n {{ cns_namespace }} -o jsonpath='{.items[0].metadata.name}' -l glusterfs=storage-pod) -- gluster vol info {{ pv_volume_path.stdout }}
      changed_when: false

    - name: check health of postgres-db deployment
      shell: oc get rc -o jsonpath='{$.items[?(@.spec.selector.name=="postgresql")].status.readyReplicas}'
      register: pgsql_db_rc_check
      failed_when: pgsql_db_rc_check.stdout != "1"
      changed_when: false

    - name: check health of rails-app deployment
      shell: oc get rc -o jsonpath='{$.items[?(@.spec.selector.name=="rails-pgsql-persistent")].status.readyReplicas}'
      register: rails_app_rc_check
      failed_when: rails_app_rc_check.stdout != "1"
      changed_when: false

- name: Deploy RWX Persistent Storage Application
  hosts: localhost
  tags:
    - rwx_example
    - rwx_deploy
  tasks:
    - include_vars: /opt/lab/environment.yml

    - set_fact:
        cns_namespace: "{{ CNS_NAMESPACE }}"
        cns_storageclass: "{{ CNS_STORAGECLASS }}"
        new_cns_nodes_internal_fqdn:
          - "{{NODE4_INTERNAL_FQDN}}"
          - "{{NODE5_INTERNAL_FQDN}}"
          - "{{NODE6_INTERNAL_FQDN}}"
        heketi_resturl: "http://heketi-storage-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}"
        heketi_admin_pw: "{{ HEKETI_ADMIN_PW }}"
        cns_storageclass2: "{{CNS_STORAGECLASS2}}"
        ocp_routing_suffix: "{{OCP_ROUTING_SUFFIX}}"
        node_brick_device: "{{ NODE_BRICK_DEVICE }}"
        node_brick_device2: "{{ NODE_BRICK_DEVICE2 }}"
        node4_internal_fqdn: " {{ NODE4_INTERNAL_FQDN }}"

    - name: log back in as fancyuser1
      shell: oc login -u 'fancyuser1' -p 'openshift'

    - name: create new project called 'my-shared-storage'
      shell: oc new-project my-shared-storage

    - name: deploy file-uploader app
      shell: oc new-app openshift/php:7.0~https://github.com/christianh814/openshift-php-upload-demo --name=file-uploader

    - name: wait for php app to be ready
      shell: oc get dc file-uploader -o jsonpath='{.status.availableReplicas}'
      register: php_app_check
      until: php_app_check.stdout == "1"
      retries: 30
      delay: 10

    - name: expose php app service
      shell: oc expose svc/file-uploader

    - name: scale php app
      shell: oc scale dc/file-uploader --replicas=3

    - name: wait for php app to be scaled to 3 pods
      shell: oc get dc file-uploader -o jsonpath='{.status.availableReplicas}'
      register: php_app_scale_check
      until: php_app_scale_check.stdout == "3"
      retries: 30
      delay: 10

    - name: add pvc to app deploymentconfig
      shell: oc volume dc/file-uploader --add --name=my-shared-storage -t pvc --claim-mode=ReadWriteMany --claim-size=1Gi --claim-name=my-shared-storage --mount-path=/opt/app-root/src/uploaded

    - name: wait for php app replication controller to be updated
      shell: oc get rc -o jsonpath='{$.items[?(@.spec.selector.deploymentconfig=="file-uploader")].spec.template.spec.volumes[?(@.persistentVolumeClaim.claimName=="my-shared-storage")].name}'
      register: php_app_update_rc_check
      until: '"shared-storage" in php_app_update_rc_check.stdout'
      retries: 30
      delay: 10

    - name: wait for php app to be fully redeployed
      shell: oc get rc file-uploader-$(oc get dc file-uploader -o jsonpath='{.status.latestVersion}') -o jsonpath='{.status.availableReplicas}'
      register: php_app_update_check
      until: php_app_update_check.stdout == "3"
      retries: 30
      delay: 10

    # check current capacity of 1Gi of "uploaded" directory in the pods

    # extend PVC to 5Gi

- name: Verify RWX Persistent Storage Application
  hosts: localhost
  tags:
    - rwx_example
    - rwx_verify
  tasks:
    - include_vars: /opt/lab/environment.yml

    - set_fact:
        cns_namespace: "{{ CNS_NAMESPACE }}"
        cns_storageclass: "{{ CNS_STORAGECLASS }}"
        new_cns_nodes_internal_fqdn:
          - "{{NODE4_INTERNAL_FQDN}}"
          - "{{NODE5_INTERNAL_FQDN}}"
          - "{{NODE6_INTERNAL_FQDN}}"
        heketi_resturl: "http://heketi-storage-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}"
        heketi_admin_pw: "{{ HEKETI_ADMIN_PW }}"
        cns_storageclass2: "{{CNS_STORAGECLASS2}}"
        ocp_routing_suffix: "{{OCP_ROUTING_SUFFIX}}"
        node_brick_device: "{{ NODE_BRICK_DEVICE }}"
        node_brick_device2: "{{ NODE_BRICK_DEVICE2 }}"
        node4_internal_fqdn: " {{ NODE4_INTERNAL_FQDN }}"

    - name: login as system:admin
      command: oc login -u system:admin

    - name: change to my-shared-storage project
      command: oc project my-shared-storage

    - name: check route of file-uploader sevice
      shell: oc get route/file-uploader
      changed_when: false

    - name: ensure rwx pvc exists
      shell: oc get pvc/my-shared-storage -o json
      register: get_file_uploader_pvc
      changed_when: false

    - set_fact:
        file_uploader_pvc: "{{ get_file_uploader_pvc.stdout|from_json }}"

    - name: ensure pvc is bound
      fail:
        msg: pvc is not bound
      when: file_uploader_pvc.status.phase != "Bound"

    - name: ensure pvc is issued against CNS
      assert:
        that:
          - file_uploader_pvc.metadata.annotations['volume.beta.kubernetes.io/storage-provisioner'] == 'kubernetes.io/glusterfs'
          - file_uploader_pvc.spec.storageClassName == cns_storageclass
        msg: pvc is not served from StorageClass {{ cns_storageclass }}

    - name: check pv is exists
      shell: oc get pv/{{ file_uploader_pvc.spec.volumeName }} -o json
      register: get_file_uploader_pv
      changed_when: false

    - set_fact:
        file_uploader_pv: "{{ get_file_uploader_pv.stdout|from_json }}"

    - name: check pv is provided by CNS
      assert:
        that:
          - file_uploader_pv.metadata.annotations['pv.kubernetes.io/provisioned-by'] == 'kubernetes.io/glusterfs'
          - file_uploader_pv.spec.storageClassName == cns_storageclass
        msg: pv is not provided by {{ cns_storageclass }}

    - name: ensure pv is file-uploader deployment config contains pvc
      shell: oc get dc/file-uploader -o jsonpath='{$.spec.template.spec.volumes[0].persistentVolumeClaim.claimName}'
      register: get_file_uploader_pvc_name
      failed_when: get_file_uploader_pvc_name.stdout != file_uploader_pvc.metadata.name
      changed_when: false

    # verify new capacity of 5Gi of "uploaded" directory in the pods

- name: Add disks to second CNS cluster
  hosts: localhost
  tags:
    - cns_ops
    - cns_add_disks
  tasks:
    - include_vars: /opt/lab/environment.yml

    - set_fact:
        cns_namespace: "{{ CNS_NAMESPACE }}"
        cns_storageclass: "{{ CNS_STORAGECLASS }}"
        new_cns_nodes_internal_fqdn:
          - "{{NODE4_INTERNAL_FQDN}}"
          - "{{NODE5_INTERNAL_FQDN}}"
          - "{{NODE6_INTERNAL_FQDN}}"
        heketi_resturl: "http://heketi-storage-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}"
        heketi_admin_pw: "{{ HEKETI_ADMIN_PW }}"

    - name: get id of cns infra cluster
      shell: heketi-cli cluster list --json | jq -r '.clusters[0]'
      register: get_cns_infra_cluster_id

    - set_fact:
        cns_infra_cluster_id: "{{ get_cns_infra_cluster_id.stdout }}"

    - name: get ids of cns infra cluster nodes
      shell: heketi-cli cluster info $CNS_INFRA_CLUSTER --json | jq '.nodes'
      register: get_cns_infra_cluster_nodes

    - set_fact:
        cns_infra_cluster_nodes: "{{ get_cns_infra_cluster_nodes.stdout | from_json }}"

    - debug: var=cns_infra_cluster_nodes

    # - name: add disks to infra cluster

- name: Perform CNS Maintenance
  hosts: localhost
  tags:
    - cns_maintenance_example
    - cns_maintenance
  tasks:
    - include_vars: /opt/lab/environment.yml

    - set_fact:
        cns_namespace: "{{ CNS_NAMESPACE }}"
        cns_storageclass: "{{ CNS_STORAGECLASS }}"
        new_cns_nodes_internal_fqdn:
          - "{{NODE4_INTERNAL_FQDN}}"
          - "{{NODE5_INTERNAL_FQDN}}"
          - "{{NODE6_INTERNAL_FQDN}}"
        heketi_resturl: "http://heketi-storage-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}"
        heketi_admin_pw: "{{ HEKETI_ADMIN_PW }}"
        cns_storageclass2: "{{CNS_STORAGECLASS2}}"
        ocp_routing_suffix: "{{OCP_ROUTING_SUFFIX}}"
        node_brick_device: "{{ NODE_BRICK_DEVICE }}"
        node_brick_device2: "{{ NODE_BRICK_DEVICE2 }}"
        node4_internal_fqdn: " {{ NODE4_INTERNAL_FQDN }}"

    - name: log back into cns namespace as cluster admin
      shell: oc login -u system:admin -n {{ cns_namespace }}
      changed_when: false

    - name: get heketi ID of first node of second CNS cluster
      shell: "heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} topology info | grep -B4 {{ node4_internal_fqdn }} | awk 'match($0, /Node Id: ([a-z0-9].*)/, result) { print result[1] }'"
      register: heketi_node_4_query

    - set_fact:
        heketi_node_4_id: "{{ heketi_node_4_query.stdout }}"

    - name: get heketi ID of first device of this node
      shell: "heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} node info {{ heketi_node_4_id }} | grep {{node_brick_device}} | awk 'match($0, /Id:([a-z0-9].*) Name:/, result) { print result[1] }'"
      register: heketi_node_4_device_query

    - set_fact:
        heketi_node_4_device_id: "{{ heketi_node_4_device_query.stdout }}"

    - name: disable the device
      shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} device disable {{ heketi_node_4_device_id }}

    - name: remove the device
      shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} device remove {{ heketi_node_4_device_id }}

    - name: delete the device
      shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} device delete {{ heketi_node_4_device_id }}

- name: Verify CNS Maintenance
  hosts: localhost
  tags:
    - cns_maintenance_example
    - cns_maintenance_verify
  tasks:
    - include_vars: /opt/lab/environment.yml

    - set_fact:
        cns_namespace: "{{ CNS_NAMESPACE }}"
        cns_storageclass: "{{ CNS_STORAGECLASS }}"
        new_cns_nodes_internal_fqdn:
          - "{{NODE4_INTERNAL_FQDN}}"
          - "{{NODE5_INTERNAL_FQDN}}"
          - "{{NODE6_INTERNAL_FQDN}}"
        heketi_resturl: "http://heketi-storage-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}"
        heketi_admin_pw: "{{ HEKETI_ADMIN_PW }}"
        cns_storageclass2: "{{CNS_STORAGECLASS2}}"
        ocp_routing_suffix: "{{OCP_ROUTING_SUFFIX}}"
        node_brick_device: "{{ NODE_BRICK_DEVICE }}"
        node_brick_device2: "{{ NODE_BRICK_DEVICE2 }}"
        node4_internal_fqdn: " {{ NODE4_INTERNAL_FQDN }}"

    - name: get heketi ID of first node of second CNS cluster
      shell: "heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} topology info | grep -B4 {{ node4_internal_fqdn }} | awk 'match($0, /Node Id: ([a-z0-9].*)/, result) { print result[1] }'"
      register: heketi_node_4_query
      changed_when: false

    - set_fact:
        heketi_node_4_id: "{{ heketi_node_4_query.stdout }}"

    - name: check that first device has been removed of first node of second cns cluster
      shell: heketi-cli --user=admin --secret {{ heketi_admin_pw }} --server {{ heketi_resturl }} node info {{ heketi_node_4_id }}
      register: cluster2_node4
      failed_when: node_brick_device in cluster2_node4.stdout
      changed_when: false

- name: Put Registry on Persistent Storage
  hosts: localhost
  tags:
    - registry_storage_example
    - registry_storage
  tasks:
    - include_vars: /opt/lab/environment.yml

    - set_fact:
        cns_namespace: "{{ CNS_NAMESPACE }}"
        cns_storageclass: "{{ CNS_STORAGECLASS }}"
        new_cns_nodes_internal_fqdn:
          - "{{NODE4_INTERNAL_FQDN}}"
          - "{{NODE5_INTERNAL_FQDN}}"
          - "{{NODE6_INTERNAL_FQDN}}"
        heketi_resturl: "http://heketi-storage-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}"
        heketi_admin_pw: "{{ HEKETI_ADMIN_PW }}"
        cns_storageclass2: "{{CNS_STORAGECLASS2}}"
        ocp_routing_suffix: "{{OCP_ROUTING_SUFFIX}}"
        node_brick_device: "{{ NODE_BRICK_DEVICE }}"
        node_brick_device2: "{{ NODE_BRICK_DEVICE2 }}"
        node4_internal_fqdn: " {{ NODE4_INTERNAL_FQDN }}"

    - name: log back in as cluster admin into default namespace
      shell: oc login -u system:admin -n default

    - name: update registry deployment
      shell: oc volume dc/docker-registry --add --name=registry-storage -t pvc --claim-mode=ReadWriteMany --claim-size=5Gi --claim-name=registry-storage --overwrite

    - name: wait for pvc to be bound
      shell: oc get pvc/registry-storage -o jsonpath='{$.status.phase}'
      register: registry_pvc
      until: registry_pvc.stdout == "Bound"
      retries: 6
      delay: 5

    - name: wait for registry deployment to be ready
      shell: oc get rc -o jsonpath='{$.items[?(@.spec.selector.deploymentconfig=="docker-registry")].status.readyReplicas}'
      register: registry_rc_check
      until: registry_rc_check.stdout == "1"
      retries: 30
      delay: 10

      # without retry we suffer from the following error: Scaling the resource failed with: Operation cannot be fulfilled on deploymentconfigs "docker-registry": the object has been modified; please apply your changes to the latest version and try again; Current resource version Unknown
    - name: scale the registry deployment to 3
      shell: oc scale dc/docker-registry --replicas=3
      register: scale_dc
      until: scale_dc|success
      retries: 5
      delay: 10

    - name: wait for registry to be scaled to 3 pods
      shell: oc get rc docker-registry-$(oc get dc docker-registry -o jsonpath='{.status.latestVersion}') -o jsonpath='{.status.availableReplicas}'
      register: registry_scale_check
      until: registry_scale_check.stdout == "3"
      retries: 30
      delay: 10

- name: Verify Registry Persistent Storage
  hosts: localhost
  tags:
    - registry_storage_example
    - registry_storage_verify
  tasks:
    - include_vars: /opt/lab/environment.yml

    - set_fact:
        cns_namespace: "{{ CNS_NAMESPACE }}"
        cns_storageclass: "{{ CNS_STORAGECLASS }}"
        new_cns_nodes_internal_fqdn:
          - "{{NODE4_INTERNAL_FQDN}}"
          - "{{NODE5_INTERNAL_FQDN}}"
          - "{{NODE6_INTERNAL_FQDN}}"
        heketi_resturl: "http://heketi-storage-{{CNS_NAMESPACE}}.{{OCP_ROUTING_SUFFIX}}"
        heketi_admin_pw: "{{ HEKETI_ADMIN_PW }}"
        cns_storageclass2: "{{CNS_STORAGECLASS2}}"
        ocp_routing_suffix: "{{OCP_ROUTING_SUFFIX}}"
        node_brick_device: "{{ NODE_BRICK_DEVICE }}"
        node_brick_device2: "{{ NODE_BRICK_DEVICE2 }}"
        node4_internal_fqdn: " {{ NODE4_INTERNAL_FQDN }}"

    - name: check registry pvc from CNS
      shell: oc get pvc/registry-storage -o json
      register: get_registry_pvc
      changed_when: false

    - set_fact:
        registry_pvc: "{{ get_registry_pvc.stdout|from_json }}"

    - name: ensure registry pv comes from registry pvc
      shell: oc get dc/docker-registry -o jsonpath='{$.spec.template.spec.volumes[0].persistentVolumeClaim.claimName}'
      register: get_registry_pvc_name
      failed_when: get_registry_pvc_name.stdout != registry_pvc.metadata.name
